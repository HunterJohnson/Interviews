# Data science stuff to review


Classification (categorical / Discrete)

Regression (continuous)
-------------------------------------------------------
Logistic regression (categorical|discrete output )

Linear regression (continuous output / dependent var)
-----------------------------------------------------------

Supervised Learning

Supervised learning uses a set of example pairs (x,y) & the aim is to find a fn F : x--> y
in the allowed class of functions that matches the examples.
A commonly used cost is the mean-squared error, which tries to minimize
the average squared error between the network's output, f(x), and the target value y.... 
over all the example pairs. Minimizing this cost using gradient descent for
the class of neural networks called multilayer perceptrons (MLP),
produces the backpropagation algorithm for training neural networks.

Tasks that fall within the paradigm of supervised learning are pattern recognition 
(also known as classification) and regression (also known as function approximation). 

--------------------------------------------------------------

In unsupervised learning, some data x is given and the cost function to be minimized, 
that can be any function of the data x & the networks output f.
The cost function is dependent on the task (the model domain) and any a priori
assumptions (the implicit properties of the model, its parameters and the observed variables).
Tasks that fall within the paradigm of unsupervised learning are in general 
estimation problems; the applications include clustering, the estimation
of statistical distributions, compression and filtering.
-----------------------------------------------------------

Reinforcement Learning

In reinforcement learning, data x are usually not given, but generated by an agent's
interactions with the environment. At each point in time t, the agent performs an action 
y_{t} and the environment generates an observation x_{t} and an instantaneous cost 
 c_{t}, according to some (usually unknown) dynamics. The aim is to discover a 
 policy for selecting actions that minimizes some measure of a long-term cost,
 e.g., the expected cumulative cost. The environment's dynamics and the
 long-term cost for each policy are usually unknown, but can be estimated.
--------------------------------------------------------------------------------------------

------ MODELS ---------
SVM (supervised) - A Support Vector Machine (SVM) is a discriminative classifier formally 
defined by a separating hyperplane. In other words, given labeled training data 
(supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples

K-means (unsupervised) - k-means clustering is a method of vector quantization, originally 
from signal processing, that is popular for cluster analysis in data mining.
k-means clustering aims to partition n observations into k clusters in which each 
observation belongs to the cluster with the nearest mean, serving as a prototype
of the cluster. This results in a partitioning of the data space into Voronoi cells.
The problem is computationally difficult (NP-hard); however, there are efficient
heuristic algorithms that are commonly employed and converge quickly to a local optimum. 

KNN (supervised) - In pattern recognition, the k-nearest neighbors algorithm (k-NN)
is a non-parametric method used for classification and regression.[1] In both
cases, the input consists of the k closest training examples in the feature space. 
The output depends on whether k-NN is used for classification or regression:
In k-NN classification, the output is a class membership. An object is classified 
by a majority vote of its neighbors, with the object being assigned to the
class most common among its k nearest neighbors (k is a positive integer,
typically small). If k = 1, then the object is simply assigned to the class
of that single nearest neighbor.
In k-NN regression, the output is the property value for the object. 
This value is the average of the values of its k nearest neighbors.
****Sensitive to the local structure of the data

Neural Networks
--->
An ANN is based on a collection of connected units or nodes called artificial
neurons which loosely model the neurons in a biological brain. Each connection, 
like the synapses in a biological brain, can transmit a signal from one
artificial neuron to another. An artificial neuron that receives a signal
can process it and then signal additional artificial neurons connected to it.

----> backpropagation

Stochastic gradient descent (often shortened to SGD), also known as
incremental gradient descent, is an iterative method for optimizing 
a differentiable objective function, a stochastic approximation of 
gradient descent optimization. It is called stochastic because samples
are selected randomly (or shuffled) instead of as a single group
(as in standard gradient descent) or in the order they appear in the training set.

**SGD: logistic regression, SVM, ANN

--------------------------------------------------------------------------
Hidden Markov Model (unsupervised)

Hidden Markov Model (HMM) is a statistical Markov model in which the system
being modeled is assumed to be a Markov process with unobserved (i.e. hidden) states.

The hidden Markov model can be represented as the simplest dynamic Bayesian network. 
The mathematics behind the HMM were developed by L. E. Baum and coworkers.
HMM is closely related to earlier work on the optimal nonlinear filtering 
problem by Ruslan L. Stratonovich,[6] who was the first to describe the forward-backward procedure.

In simpler Markov models (like a Markov chain), the state is directly visible
to the observer, and therefore the state transition probabilities are the
only parameters, while in the hidden Markov model, the state is not directly
visible, but the output (in the form of data or "token" in the following),
dependent on the state, is visible. Each state has a probability distribution
over the possible output tokens.

---------------------------------------------------------------------
Bayesian inference is a method of statistical inference in which Bayes' theorem
is used to update the probability for a hypothesis as more evidence or information becomes available.

- Bayesian analysis is a statistical procedure which endeavors to estimate
parameters of an underlying distribution based on the observed distribution.

- A Bayesian network, Bayes network, belief network, Bayes(ian) model or
probabilistic directed acyclic graphical model is a probabilistic 
graphical model (a type of statistical model) that represents a set 
of variables and their conditional dependencies via a directed acyclic graph (DAG).

Bayes Rule: P(A|B) = P(B|A) P(A)
                    --------------
                        P(B)

-------------------------------------------------
-------------------------------------------------
Dimensionality reduction is the process of reducing the number of random variables under consideration,
by obtaining a set of principal variables. It can be divided into feature selection and feature extraction.

Dimensionality Reduction is an important factor in predictive modeling. Various proposed methods
have introduced different approaches to do so by either graphically or by various other 
methods like filtering, wrapping or embedding. However, most of these approaches are
based on some threshold values and benchmark algorithms that determine the optimality
of the features in the dataset.

One motivation for dimensionality reduction is that higher dimensional data sets increase
the time complexity and also the space required will be more. Also, all the features
in the dataset might not be useful. Some may contribute no information at all, while 
some may contribute similar information as the other features. Selecting the optimal 
set of features will help us hence reduce the space and time complexity as well as
increase the accuracy or purity of classification (or regression) and clustering 
(or association) for supervised and unsupervised learning respectively.
---------------------------------------------

Ways to prevent overfitting:

Cross- Validation: A standard way to find out-of-sample prediction error is to use 5-fold cross validation.
---> test the modelâ€™s ability to predict new data that was not used in estimating it, in order to
flag problems like overfitting. and to give an insight on how the model will generalize to 
 an independent dataset (i.e., an unknown dataset, for instance from a real problem. 
 (Split data into train/validate/test)
Early Stopping: Its rules provide us the guidance as to how many iterations can be run
                before learner begins to over-fit.
** more data
** Use data augmentation
** weight decay (minimize risk of gradient descent / SGD getting stuck in local optima)
** Use architectures that generalize well, reduce complexity
Add regularization (mostly dropout, L1/L2 regularization are also possible)
 ----> 1) to avoid overfitting by not generating high coefficients for predictors that are sparse.
       2) to stabilize the estimates especially when there's collinearity in the data.  
---> L1 regularization (LASSO --> least absolute shrinkage & selection operator) 
(good for sparse signals, incorporates variable selection w/ many weights going to 0 after regularization)

---> L2 regularization (ridge regression) 
    (good for minimizing prediction error, good for when collinearity exists)
-----> In statistics, multicollinearity (also collinearity) is a phenomenon in
which one predictor variable in a multiple regression model can be linearly 
predicted from the others with a substantial degree of accuracy.

 L1-norm does not have an analytical solution, but L2-norm does.
 This allows the L2-norm solutions to be calculated computationally efficiently.
