# Data science stuff to review


Classification (categorical / Discrete)

Regression (continuous)
-------------------------------------------------------
Logistic regression (categorical|discrete output )

Linear regression (continuous output / dependent var)
-----------------------------------------------------------

Supervised Learning

Supervised learning uses a set of example pairs (x,y) & the aim is to find a fn F : x--> y
in the allowed class of functions that matches the examples.
A commonly used cost is the mean-squared error, which tries to minimize
the average squared error between the network's output, f(x), and the target value y.... 
over all the example pairs. Minimizing this cost using gradient descent for
the class of neural networks called multilayer perceptrons (MLP),
produces the backpropagation algorithm for training neural networks.

Tasks that fall within the paradigm of supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). 
--------------------------------------------------------------
In unsupervised learning, some data x is given and the cost function to be minimized, 
that can be any function of the data x & the networks output f.
The cost function is dependent on the task (the model domain) and any a priori
assumptions (the implicit properties of the model, its parameters and the observed variables).
Tasks that fall within the paradigm of unsupervised learning are in general 
estimation problems; the applications include clustering, the estimation
of statistical distributions, compression and filtering.
-----------------------------------------------------------


------ MODELS ---------
SVM (supervised) - A Support Vector Machine (SVM) is a discriminative classifier formally 
defined by a separating hyperplane. In other words, given labeled training data 
(supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples

K-means (unsupervised) - k-means clustering is a method of vector quantization, originally 
from signal processing, that is popular for cluster analysis in data mining.
k-means clustering aims to partition n observations into k clusters in which each 
observation belongs to the cluster with the nearest mean, serving as a prototype
of the cluster. This results in a partitioning of the data space into Voronoi cells.
The problem is computationally difficult (NP-hard); however, there are efficient heuristic algorithms that are commonly employed and converge quickly to a local optimum. 

KNN (supervised) - In pattern recognition, the k-nearest neighbors algorithm (k-NN)
is a non-parametric method used for classification and regression.[1] In both
cases, the input consists of the k closest training examples in the feature space. 
The output depends on whether k-NN is used for classification or regression:
In k-NN classification, the output is a class membership. An object is classified 
by a majority vote of its neighbors, with the object being assigned to the
class most common among its k nearest neighbors (k is a positive integer,
typically small). If k = 1, then the object is simply assigned to the class
of that single nearest neighbor.
In k-NN regression, the output is the property value for the object. 
This value is the average of the values of its k nearest neighbors.
****Sensitive to the local structure of the data

Neural Networks
--->
An ANN is based on a collection of connected units or nodes called artificial
neurons which loosely model the neurons in a biological brain. Each connection, 
like the synapses in a biological brain, can transmit a signal from one
artificial neuron to another. An artificial neuron that receives a signal
can process it and then signal additional artificial neurons connected to it.

----> backpropagation

Hidden Markov Model

-------------------------------------------------
Dimensionality reduction is the process of reducing the number of random variables under consideration,
by obtaining a set of principal variables. It can be divided into feature selection and feature extraction.

Dimensionality Reduction is an important factor in predictive modeling. Various proposed methods
have introduced different approaches to do so by either graphically or by various other 
methods like filtering, wrapping or embedding. However, most of these approaches are
based on some threshold values and benchmark algorithms that determine the optimality
of the features in the dataset.

One motivation for dimensionality reduction is that higher dimensional data sets increase
the time complexity and also the space required will be more. Also, all the features
in the dataset might not be useful. Some may contribute no information at all, while 
some may contribute similar information as the other features. Selecting the optimal 
set of features will help us hence reduce the space and time complexity as well as
increase the accuracy or purity of classification (or regression) and clustering 
(or association) for supervised and unsupervised learning respectively.
---------------------------------------------

Ways to prevent overfitting:

Cross- Validation: A standard way to find out-of-sample prediction error is to use 5-fold cross validation.
---> test the modelâ€™s ability to predict new data that was not used in estimating it, in order to
flag problems like overfitting. and to give an insight on how the model will generalize to 
 an independent dataset (i.e., an unknown dataset, for instance from a real problem. 
 (Split data into train/validate/test)
Early Stopping: Its rules provide us the guidance as to how many iterations can be run
                before learner begins to over-fit.
** more data
** Use data augmentation
** weight decay (minimize risk of gradient descent / SGD getting stuck in local optima)
** Use architectures that generalize well, reduce complexity
Add regularization (mostly dropout, L1/L2 regularization are also possible)
 ----> 1) to avoid overfitting by not generating high coefficients for predictors that are sparse.
       2) to stabilize the estimates especially when there's collinearity in the data.  
---> L1 regularization (LASSO --> least absolute shrinkage & selection operator) 
(good for sparse signals, incorporates variable selection w/ many weights going to 0 after regularization)

---> L2 regularization (ridge regression)  (good for minimizing prediction error, good for when collinearity exists)
-----> In statistics, multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.

 L1-norm does not have an analytical solution, but L2-norm does.
 This allows the L2-norm solutions to be calculated computationally efficiently.
